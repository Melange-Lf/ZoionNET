{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import joblib\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from torchinfo import summary\n",
    "# import torchmetrics\n",
    "\n",
    "# import utils\n",
    "# import engine\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self, # bug in dropout_specification of gru, do contiguous memory after permutate\n",
    "                 \n",
    "                # weight initialization was using their ref 46\n",
    "\n",
    "                 # for input they had 40 mel bands, time steps varied\n",
    "                 input_shape, # (Frequecies, Timesteps, Channels)\n",
    "\n",
    "                 # cnn layers, do grid search 1, 2, 3, 4\n",
    "                 #      the kernel, best 5, next 3\n",
    "                 #      the filters used by them 96\n",
    "                 #      pool sizes 2x1 non overlapping stride\n",
    "                 #      dropout after cnn was used with 0.25\n",
    "                 cnn_layers=3,kernels=(5, 5, 5), filters=(96, 96, 96), pool_sizes=(2, 2, 2),\n",
    "                 cnn_dropout=(0.25, 0.25, 0.25),\n",
    "                # rnn layers, do grid search 1, 2, 3\n",
    "                #       their dropout was done in custom with ref 35\n",
    "                #           I have just used the same constant for prob, and implemented the inbuilt pytorch dropout for rnns\n",
    "                 rnn_layers=2, rnn_hidden=(256, 256),\n",
    "                 rnn_dropout=(0.25,),\n",
    "\n",
    "                # fnn layer hidden units is taken from their ref 21, where it shares a lot of simularity, \n",
    "                #           no real basis for the hidden units though, as the arch in ref 21 is very different\n",
    "                #           do grid search then 128, 256, 512, 1024, 2048, 4096(overkill)\n",
    "                #       dropout i just used the same constant\n",
    "                 fnn_layers=1, fnn_hidden=(1024,), fnn_dropout=(0.25,),\n",
    "                 output_shape=5):\n",
    "        \n",
    "        super(CRNN, self).__init__()\n",
    "\n",
    "        self.time_steps = input_shape[1]\n",
    "\n",
    "        # =============================================================================\n",
    "\n",
    "        self.cnn_blocks = nn.Sequential()\n",
    "        in_channels = input_shape[2]\n",
    "        freq = input_shape[0]\n",
    "\n",
    "        for i in range(cnn_layers):\n",
    "            self.cnn_blocks.add_module(f'cnn_{i}', nn.Conv2d(in_channels, filters[i], kernel_size=kernels[i], padding=\"same\"))\n",
    "            self.cnn_blocks.add_module(f'relu_{i}', nn.ReLU())\n",
    "            self.cnn_blocks.add_module(f'pool_{i}', nn.MaxPool2d(kernel_size=(pool_sizes[i], 1), stride=(pool_sizes[i], 1)))\n",
    "            self.cnn_blocks.add_module(f'batchnorm_{i}', nn.BatchNorm2d(filters[i]))\n",
    "            self.cnn_blocks.add_module(f'dropout_{i}', nn.Dropout(cnn_dropout[i]))\n",
    "\n",
    "            in_channels = filters[i]\n",
    "            freq = (freq - (pool_sizes[i] - 1) -1)//pool_sizes[i] + 1\n",
    "\n",
    "        self.freq = freq\n",
    "        self.channels = filters[-1]\n",
    "\n",
    "        # ==============================================================================\n",
    "\n",
    "        self.rnn_input_size = self.freq * self.channels\n",
    "\n",
    "        self.rnn = nn.Sequential()\n",
    "        input_size = self.rnn_input_size\n",
    "        for i in range(rnn_layers):\n",
    "            self.rnn.add_module(f'gru_{i}', nn.GRU(input_size, rnn_hidden[i], batch_first=True, dropout=rnn_dropout[i] if i < rnn_layers - 1 else 0))\n",
    "            input_size = rnn_hidden[i]\n",
    "\n",
    "        # ===============================================================================\n",
    "\n",
    "        input_size = self.time_steps*rnn_hidden[-1]\n",
    "\n",
    "        self.fnn_blocks = nn.Sequential()\n",
    "        for i in range(fnn_layers):\n",
    "            self.fnn_blocks.add_module(f'fc_{i}', nn.Linear(input_size, fnn_hidden[i]))\n",
    "            self.fnn_blocks.add_module(f'relu_{i}', nn.ReLU())\n",
    "            self.fnn_blocks.add_module(f'batchnorm_{i}', nn.BatchNorm1d(fnn_hidden[i]))\n",
    "            self.fnn_blocks.add_module(f'dropout_{i}', nn.Dropout(fnn_dropout[i]))\n",
    "            input_size = fnn_hidden[i]\n",
    "\n",
    "        self.output_layer = nn.Linear(input_size, output_shape)\n",
    "\n",
    "    def forward(self, x): \n",
    "\n",
    "        x = x.permute(0, 3, 1, 2) # (B, C_in, F, T)\n",
    "        \n",
    "          \n",
    "        # print(\"cnn input: \", x.shape)\n",
    "        for name, layer in self.cnn_blocks.named_children(): # outs = # (B, C_out, F', T)\n",
    "            x = layer(x)\n",
    "            # print(f\"{name}: \", x.shape)\n",
    "        \n",
    "        # print(\"after conv: \", x.shape)\n",
    "\n",
    "        B, C_out, F_prime, T = x.size()\n",
    "        x = x.permute(0, 3, 1, 2)  # (B, T, C_out, F')\n",
    "\n",
    "        x = x.reshape(B, T, F_prime * C_out)  # (B, T, F' * C_out)\n",
    "\n",
    "        # print(\"for rnn: \", x.shape)\n",
    "\n",
    "        for name, layer in self.rnn.named_children(): # outs: (B, T, H_out)\n",
    "            x, _ = layer(x)\n",
    "            # print(f\"{name} \", x.shape)\n",
    "\n",
    "        B, T, H_out = x.size()\n",
    "        # print(\"after rnn: \", x.shape)\n",
    "\n",
    "        x = x.reshape(B, T*H_out)\n",
    "\n",
    "        # print(\"for fnn: \", x.shape)\n",
    "\n",
    "        x = self.fnn_blocks(x)\n",
    "\n",
    "        # print(\"after fnn: \", x.shape)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        # print(\"outs: \", x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vansh\\Documents\\code\\Py\\actual\\Ai\\IEEE comsoc\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "CRNN                                     [64, 40, 128, 1]          [64, 5]                   --\n",
       "├─Sequential: 1-1                        --                        --                        --\n",
       "│    └─Conv2d: 2-1                       [64, 1, 40, 128]          [64, 96, 40, 128]         2,496\n",
       "│    └─ReLU: 2-2                         [64, 96, 40, 128]         [64, 96, 40, 128]         --\n",
       "│    └─MaxPool2d: 2-3                    [64, 96, 40, 128]         [64, 96, 20, 128]         --\n",
       "│    └─BatchNorm2d: 2-4                  [64, 96, 20, 128]         [64, 96, 20, 128]         192\n",
       "│    └─Dropout: 2-5                      [64, 96, 20, 128]         [64, 96, 20, 128]         --\n",
       "│    └─Conv2d: 2-6                       [64, 96, 20, 128]         [64, 96, 20, 128]         230,496\n",
       "│    └─ReLU: 2-7                         [64, 96, 20, 128]         [64, 96, 20, 128]         --\n",
       "│    └─MaxPool2d: 2-8                    [64, 96, 20, 128]         [64, 96, 10, 128]         --\n",
       "│    └─BatchNorm2d: 2-9                  [64, 96, 10, 128]         [64, 96, 10, 128]         192\n",
       "│    └─Dropout: 2-10                     [64, 96, 10, 128]         [64, 96, 10, 128]         --\n",
       "│    └─Conv2d: 2-11                      [64, 96, 10, 128]         [64, 96, 10, 128]         230,496\n",
       "│    └─ReLU: 2-12                        [64, 96, 10, 128]         [64, 96, 10, 128]         --\n",
       "│    └─MaxPool2d: 2-13                   [64, 96, 10, 128]         [64, 96, 5, 128]          --\n",
       "│    └─BatchNorm2d: 2-14                 [64, 96, 5, 128]          [64, 96, 5, 128]          192\n",
       "│    └─Dropout: 2-15                     [64, 96, 5, 128]          [64, 96, 5, 128]          --\n",
       "├─Sequential: 1-2                        --                        --                        --\n",
       "│    └─GRU: 2-16                         [64, 128, 480]            [64, 128, 256]            566,784\n",
       "│    └─GRU: 2-17                         [64, 128, 256]            [64, 128, 256]            394,752\n",
       "├─Sequential: 1-3                        [64, 32768]               [64, 1024]                --\n",
       "│    └─Linear: 2-18                      [64, 32768]               [64, 1024]                33,555,456\n",
       "│    └─ReLU: 2-19                        [64, 1024]                [64, 1024]                --\n",
       "│    └─BatchNorm1d: 2-20                 [64, 1024]                [64, 1024]                2,048\n",
       "│    └─Dropout: 2-21                     [64, 1024]                [64, 1024]                --\n",
       "├─Linear: 1-4                            [64, 1024]                [64, 5]                   5,125\n",
       "===================================================================================================================\n",
       "Total params: 34,988,229\n",
       "Trainable params: 34,988,229\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 67.49\n",
       "===================================================================================================================\n",
       "Input size (MB): 1.31\n",
       "Forward/backward pass size (MB): 695.21\n",
       "Params size (MB): 139.95\n",
       "Estimated Total Size (MB): 836.47\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking 40 bands, arbitary time steps\n",
    "test = CRNN(input_shape=(40, 128, 1), output_shape=5)\n",
    "summary(test, input_size=(64, 40, 128, 1), col_names=['input_size', 'output_size', 'num_params'])\n",
    "\n",
    "# NOTE: main contributor to the parameter number is: concatenation of outputs from gru and feeding to linear layer\n",
    "# need reduction methodology here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <class 'torch.Tensor'>\n",
    "# after conv:  torch.Size([64, 256, 12, 88])\n",
    "# for rnn:  torch.Size([64, 88, 3072])\n",
    "# after rnn:  torch.Size([64, 88, 256])\n",
    "# ---------------------------------------------------------------------------\n",
    "# RuntimeError    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN2(nn.Module):\n",
    "    def __init__(self, # bug in dropout_specification of gru, do contiguous memory after permutate\n",
    "                 \n",
    "                # weight initialization was using their ref 46\n",
    "\n",
    "                 # for input they had 40 mel bands, time steps varied\n",
    "                 input_shape, # (Frequecies, Timesteps, Channels)\n",
    "\n",
    "                 # cnn layers, do grid search 1, 2, 3, 4\n",
    "                 #      the kernel, best 5, next 3\n",
    "                 #      the filters used by them 96\n",
    "                 #      pool sizes 2x1 non overlapping stride\n",
    "                 #      dropout after cnn was used with 0.25\n",
    "                 cnn_layers=3,kernels=(5, 5, 5), filters=(96, 96, 96), pool_sizes=(2, 2, 2),\n",
    "                 cnn_dropout=(0.25, 0.25, 0.25),\n",
    "                # rnn layers, do grid search 1, 2, 3\n",
    "                #       their dropout was done in custom with ref 35\n",
    "                #           I have just used the same constant for prob, and implemented the inbuilt pytorch dropout for rnns\n",
    "                 rnn_layers=2, rnn_hidden=(256, 256),\n",
    "                 rnn_dropout=(0.25,),\n",
    "\n",
    "                # fnn layer hidden units is taken from their ref 21, where it shares a lot of simularity, \n",
    "                #           no real basis for the hidden units though, as the arch in ref 21 is very different\n",
    "                #           do grid search then 128, 256, 512, 1024, 2048, 4096(overkill)\n",
    "                #       dropout i just used the same constant\n",
    "                # encoder out nodes: refers to the output nodes of the encoder layer\n",
    "                #           the encoder layer is applied on each of the [time_steps] outputs of the rnn layer\n",
    "                #           effectively transforms from the rnn output from (time_steps, rnn_hidden[-1]) to (time_steps, enc_out_nodes)\n",
    "                encoder_out_nodes = 16,\n",
    "                fnn_layers=1, fnn_hidden=(1024,), fnn_dropout=(0.25,),\n",
    "                output_shape=5):\n",
    "        \n",
    "        super(CRNN2, self).__init__()\n",
    "\n",
    "        self.time_steps = input_shape[1]\n",
    "\n",
    "        # =============================================================================\n",
    "\n",
    "        self.cnn_blocks = nn.Sequential()\n",
    "        in_channels = input_shape[2]\n",
    "        freq = input_shape[0]\n",
    "\n",
    "        for i in range(cnn_layers):\n",
    "            self.cnn_blocks.add_module(f'cnn_{i}', nn.Conv2d(in_channels, filters[i], kernel_size=kernels[i], padding=\"same\"))\n",
    "            self.cnn_blocks.add_module(f'relu_{i}', nn.ReLU())\n",
    "            self.cnn_blocks.add_module(f'pool_{i}', nn.MaxPool2d(kernel_size=(pool_sizes[i], 1), stride=(pool_sizes[i], 1)))\n",
    "            self.cnn_blocks.add_module(f'batchnorm_{i}', nn.BatchNorm2d(filters[i]))\n",
    "            self.cnn_blocks.add_module(f'dropout_{i}', nn.Dropout(cnn_dropout[i]))\n",
    "\n",
    "            in_channels = filters[i]\n",
    "            freq = (freq - (pool_sizes[i] - 1) -1)//pool_sizes[i] + 1\n",
    "\n",
    "        self.freq = freq\n",
    "        self.channels = filters[-1]\n",
    "\n",
    "        # ==============================================================================\n",
    "\n",
    "        self.rnn_input_size = self.freq * self.channels\n",
    "\n",
    "        self.rnn = nn.Sequential()\n",
    "        input_size = self.rnn_input_size\n",
    "        for i in range(rnn_layers):\n",
    "            self.rnn.add_module(f'gru_{i}', nn.GRU(input_size, rnn_hidden[i], batch_first=True, dropout=rnn_dropout[i] if i < rnn_layers - 1 else 0))\n",
    "            input_size = rnn_hidden[i]\n",
    "\n",
    "        # ===============================================================================\n",
    "        # current shape: (time_steps, h_out)\n",
    "\n",
    "        input_size = (self.time_steps, rnn_hidden[-1])\n",
    "\n",
    "        self.encoder_layer = nn.Linear(input_size[1], encoder_out_nodes)\n",
    "\n",
    "        input_size = self.time_steps*encoder_out_nodes\n",
    "\n",
    "        self.fnn_blocks = nn.Sequential()\n",
    "        for i in range(fnn_layers):\n",
    "            self.fnn_blocks.add_module(f'fc_{i}', nn.Linear(input_size, fnn_hidden[i]))\n",
    "            self.fnn_blocks.add_module(f'relu_{i}', nn.ReLU())\n",
    "            self.fnn_blocks.add_module(f'batchnorm_{i}', nn.BatchNorm1d(fnn_hidden[i]))\n",
    "            self.fnn_blocks.add_module(f'dropout_{i}', nn.Dropout(fnn_dropout[i]))\n",
    "            input_size = fnn_hidden[i]\n",
    "\n",
    "        self.output_layer = nn.Linear(input_size, output_shape)\n",
    "\n",
    "    def forward(self, x): \n",
    "\n",
    "        x = x.permute(0, 3, 1, 2) # (B, C_in, F, T)\n",
    "        \n",
    "          \n",
    "        # print(\"cnn input: \", x.shape)\n",
    "        for name, layer in self.cnn_blocks.named_children(): # outs = # (B, C_out, F', T)\n",
    "            x = layer(x)\n",
    "            # print(f\"{name}: \", x.shape)\n",
    "        \n",
    "        # print(\"after conv: \", x.shape)\n",
    "\n",
    "        B, C_out, F_prime, T = x.size()\n",
    "        x = x.permute(0, 3, 1, 2)  # (B, T, C_out, F')\n",
    "\n",
    "        x = x.reshape(B, T, F_prime * C_out)  # (B, T, F' * C_out)\n",
    "\n",
    "        # print(\"for rnn: \", x.shape)\n",
    "\n",
    "        for name, layer in self.rnn.named_children(): # outs: (B, T, H_out)\n",
    "            x, _ = layer(x)\n",
    "            # print(f\"{name} \", x.shape)\n",
    "\n",
    "        B, T, H_out = x.size()\n",
    "        # print(\"after rnn: \", x.shape)\n",
    "\n",
    "\n",
    "        # =====================================\n",
    "\n",
    "        x = self.encoder_layer(x)\n",
    "\n",
    "        B, enc_nodes, H_out = x.size()\n",
    "        print(x.size())\n",
    "\n",
    "\n",
    "        x = x.reshape(B, enc_nodes*H_out)\n",
    "\n",
    "        # print(\"for fnn: \", x.shape)\n",
    "\n",
    "        x = self.fnn_blocks(x)\n",
    "\n",
    "        # print(\"after fnn: \", x.shape)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        # print(\"outs: \", x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "CRNN2                                    [64, 40, 128, 1]          [64, 5]                   --\n",
       "├─Sequential: 1-1                        --                        --                        --\n",
       "│    └─Conv2d: 2-1                       [64, 1, 40, 128]          [64, 96, 40, 128]         2,496\n",
       "│    └─ReLU: 2-2                         [64, 96, 40, 128]         [64, 96, 40, 128]         --\n",
       "│    └─MaxPool2d: 2-3                    [64, 96, 40, 128]         [64, 96, 20, 128]         --\n",
       "│    └─BatchNorm2d: 2-4                  [64, 96, 20, 128]         [64, 96, 20, 128]         192\n",
       "│    └─Dropout: 2-5                      [64, 96, 20, 128]         [64, 96, 20, 128]         --\n",
       "│    └─Conv2d: 2-6                       [64, 96, 20, 128]         [64, 96, 20, 128]         230,496\n",
       "│    └─ReLU: 2-7                         [64, 96, 20, 128]         [64, 96, 20, 128]         --\n",
       "│    └─MaxPool2d: 2-8                    [64, 96, 20, 128]         [64, 96, 10, 128]         --\n",
       "│    └─BatchNorm2d: 2-9                  [64, 96, 10, 128]         [64, 96, 10, 128]         192\n",
       "│    └─Dropout: 2-10                     [64, 96, 10, 128]         [64, 96, 10, 128]         --\n",
       "│    └─Conv2d: 2-11                      [64, 96, 10, 128]         [64, 96, 10, 128]         230,496\n",
       "│    └─ReLU: 2-12                        [64, 96, 10, 128]         [64, 96, 10, 128]         --\n",
       "│    └─MaxPool2d: 2-13                   [64, 96, 10, 128]         [64, 96, 5, 128]          --\n",
       "│    └─BatchNorm2d: 2-14                 [64, 96, 5, 128]          [64, 96, 5, 128]          192\n",
       "│    └─Dropout: 2-15                     [64, 96, 5, 128]          [64, 96, 5, 128]          --\n",
       "├─Sequential: 1-2                        --                        --                        --\n",
       "│    └─GRU: 2-16                         [64, 128, 480]            [64, 128, 256]            566,784\n",
       "│    └─GRU: 2-17                         [64, 128, 256]            [64, 128, 256]            394,752\n",
       "├─Linear: 1-3                            [64, 128, 256]            [64, 128, 16]             4,112\n",
       "├─Sequential: 1-4                        [64, 2048]                [64, 1024]                --\n",
       "│    └─Linear: 2-18                      [64, 2048]                [64, 1024]                2,098,176\n",
       "│    └─ReLU: 2-19                        [64, 1024]                [64, 1024]                --\n",
       "│    └─BatchNorm1d: 2-20                 [64, 1024]                [64, 1024]                2,048\n",
       "│    └─Dropout: 2-21                     [64, 1024]                [64, 1024]                --\n",
       "│    └─Linear: 2-22                      [64, 1024]                [64, 1024]                1,049,600\n",
       "│    └─ReLU: 2-23                        [64, 1024]                [64, 1024]                --\n",
       "│    └─BatchNorm1d: 2-24                 [64, 1024]                [64, 1024]                2,048\n",
       "│    └─Dropout: 2-25                     [64, 1024]                [64, 1024]                --\n",
       "├─Linear: 1-5                            [64, 1024]                [64, 5]                   5,125\n",
       "===================================================================================================================\n",
       "Total params: 4,586,709\n",
       "Trainable params: 4,586,709\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 65.54\n",
       "===================================================================================================================\n",
       "Input size (MB): 1.31\n",
       "Forward/backward pass size (MB): 697.31\n",
       "Params size (MB): 18.35\n",
       "Estimated Total Size (MB): 716.96\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking 40 bands, arbitary time steps\n",
    "test = CRNN2(input_shape=(40, 128, 1), fnn_layers=2, fnn_hidden=(1024,1024), fnn_dropout=(0.25,0.25), output_shape=5)\n",
    "summary(test, input_size=(64, 40, 128, 1), col_names=['input_size', 'output_size', 'num_params'])\n",
    "\n",
    "# NOTE: main contributor to the parameter number is: concatenation of outputs from gru and feeding to linear layer\n",
    "# need reduction methodology here\n",
    "\n",
    "# NOTE: That ^ has been reduced as follows: use an encoder layer that goes over each time_step's the gru outputs\n",
    "# to reduce the hidden units parameter to encoder_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
