{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9ca9de5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-06T11:56:16.298364Z",
     "iopub.status.busy": "2025-03-06T11:56:16.298073Z",
     "iopub.status.idle": "2025-03-06T11:56:17.255357Z",
     "shell.execute_reply": "2025-03-06T11:56:17.254526Z"
    },
    "papermill": {
     "duration": 0.963044,
     "end_time": "2025-03-06T11:56:17.257140",
     "exception": false,
     "start_time": "2025-03-06T11:56:16.294096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2cb2f8",
   "metadata": {
    "papermill": {
     "duration": 0.001662,
     "end_time": "2025-03-06T11:56:17.261580",
     "exception": false,
     "start_time": "2025-03-06T11:56:17.259918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# AST FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcd3b612",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T11:56:17.266660Z",
     "iopub.status.busy": "2025-03-06T11:56:17.266154Z",
     "iopub.status.idle": "2025-03-06T11:56:22.063477Z",
     "shell.execute_reply": "2025-03-06T11:56:22.062414Z"
    },
    "papermill": {
     "duration": 4.802493,
     "end_time": "2025-03-06T11:56:22.066006",
     "exception": false,
     "start_time": "2025-03-06T11:56:17.263513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ed2abb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T11:56:22.071587Z",
     "iopub.status.busy": "2025-03-06T11:56:22.071242Z",
     "iopub.status.idle": "2025-03-06T11:56:22.076038Z",
     "shell.execute_reply": "2025-03-06T11:56:22.075237Z"
    },
    "papermill": {
     "duration": 0.009003,
     "end_time": "2025-03-06T11:56:22.077325",
     "exception": false,
     "start_time": "2025-03-06T11:56:22.068322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import math\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "# from torchvision import transforms\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from PIL import Image\n",
    "\n",
    "# # -------------------------\n",
    "# # Custom Dataset for Spectrograms\n",
    "# # -------------------------\n",
    "# class SpectrogramDataset(Dataset):\n",
    "#     def __init__(self, image_dir, transform=None):\n",
    "#         self.image_dir = Path(image_dir)\n",
    "#         self.image_files = [f for f in os.listdir(image_dir) if f.endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_files)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_file = self.image_files[idx]\n",
    "#         img_path = self.image_dir / img_file\n",
    "#         image = Image.open(img_path).convert(\"L\")  # Convert to grayscale\n",
    "        \n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "\n",
    "#         return image, img_file  # Return image tensor and filename\n",
    "\n",
    "# # -------------------------\n",
    "# # Define AST Model (Modified for 3 Classes)\n",
    "# # -------------------------\n",
    "# class PatchEmbedding(nn.Module):\n",
    "#     def __init__(self, img_size=224, patch_size=16, overlap=6, embed_dim=256):\n",
    "#         super().__init__()\n",
    "#         self.proj = nn.Conv2d(in_channels=1, out_channels=embed_dim, \n",
    "#                               kernel_size=patch_size, stride=patch_size-overlap)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.proj(x)  # Shape: (batch_size, embed_dim, num_patches_h, num_patches_w)\n",
    "#         x = x.flatten(2).transpose(1, 2)  # Shape: (batch_size, num_patches, embed_dim)\n",
    "#         return x\n",
    "\n",
    "# class MultiHeadSelfAttention(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads):\n",
    "#         super().__init__()\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = embed_dim // num_heads\n",
    "#         self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "#         self.out = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         batch_size, num_patches, embed_dim = x.shape\n",
    "#         qkv = self.qkv(x).reshape(batch_size, num_patches, 3, self.num_heads, self.head_dim)\n",
    "#         q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
    "#         attn_weights = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "#         attn = F.softmax(attn_weights, dim=-1)\n",
    "#         out = (attn @ v).transpose(1, 2).reshape(batch_size, num_patches, embed_dim)\n",
    "#         return self.out(out)\n",
    "\n",
    "# class TransformerEncoderBlock(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.norm1 = nn.LayerNorm(embed_dim)\n",
    "#         self.attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "#         self.norm2 = nn.LayerNorm(embed_dim)\n",
    "#         self.mlp = nn.Sequential(\n",
    "#             nn.Linear(embed_dim, mlp_dim),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(mlp_dim, embed_dim)\n",
    "#         )\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.dropout(self.attn(self.norm1(x)))  \n",
    "#         x = x + self.dropout(self.mlp(self.norm2(x)))   \n",
    "#         return x\n",
    "\n",
    "# class AST(nn.Module):\n",
    "#     def __init__(self, img_size=224, patch_size=16, num_classes=3, embed_dim=256, \n",
    "#                  num_heads=8, depth=6, mlp_dim=512, dropout=0.1):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.patch_embed = PatchEmbedding(img_size, patch_size, overlap=6, embed_dim=embed_dim)\n",
    "#         num_patches = self.patch_embed.proj.weight.shape[-1] ** 2\n",
    "        \n",
    "#         self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "#         self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
    "        \n",
    "#         self.encoder = nn.Sequential(*[TransformerEncoderBlock(embed_dim, num_heads, mlp_dim, dropout) \n",
    "#                                        for _ in range(depth)])\n",
    "        \n",
    "#         self.norm = nn.LayerNorm(embed_dim)\n",
    "#         self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.patch_embed(x)\n",
    "#         batch_size = x.shape[0]\n",
    "#         cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
    "#         x = torch.cat([cls_token, x], dim=1)\n",
    "#         x = x + self.pos_embedding\n",
    "#         x = self.encoder(x)\n",
    "#         x = self.norm(x[:, 0])  # CLS token output\n",
    "        \n",
    "#         soft_labels = torch.sigmoid(self.head(x))  # Multi-label classification\n",
    "#         return soft_labels\n",
    "\n",
    "\n",
    "# # -------------------------\n",
    "# # Load Data and Use DataLoader\n",
    "# # -------------------------\n",
    "# # data_dir = Path(\"your_dataset_directory\")  # Change this to your dataset folder\n",
    "# # train_dir = data_dir / \"train\"\n",
    "# # csv_path = data_dir / \"train_labels.csv\"\n",
    "\n",
    "# # # Define image transformations\n",
    "# # transform = transforms.Compose([\n",
    "# #     transforms.Grayscale(num_output_channels=1),  # Ensure single-channel\n",
    "# #     transforms.ToTensor(),  # Convert to tensor\n",
    "# # ])\n",
    "\n",
    "# # # Create dataset and dataloader\n",
    "# # batch_size = 32 \n",
    "# # dataset = SpectrogramDataset(train_dir, transform=transform)\n",
    "# # dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # ast_model = AST()\n",
    "# # ast_model.eval() \n",
    "\n",
    "# # # -------------------------\n",
    "# # # Process Batches Using DataLoader\n",
    "# # # -------------------------\n",
    "# # df_records = []\n",
    "\n",
    "# # with torch.no_grad():\n",
    "# #     for batch_images, batch_filenames in dataloader:\n",
    "# #         batch_images = batch_images.unsqueeze(1)  # Add channel dimension (B, 1, H, W)\n",
    "        \n",
    "# #         soft_labels = ast_model(batch_images)  # Forward pass\n",
    "        \n",
    "# #         for filename, label in zip(batch_filenames, soft_labels):\n",
    "# #             df_records.append([filename] + label.tolist())\n",
    "\n",
    "# # # -------------------------\n",
    "# # # Save Predictions to CSV\n",
    "# # # -------------------------\n",
    "# # class_names = [\"Class_1\", \"Class_2\", \"Class_3\"]  # Modify with actual class names\n",
    "# # df = pd.DataFrame(df_records, columns=[\"Path\"] + class_names)\n",
    "# # df.to_csv(csv_path, index=False)\n",
    "\n",
    "# # print(f\"Soft labels generated! Labels saved in {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe4d849",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T10:51:22.900723Z",
     "iopub.status.busy": "2025-03-06T10:51:22.900314Z",
     "iopub.status.idle": "2025-03-06T10:51:23.654512Z",
     "shell.execute_reply": "2025-03-06T10:51:23.653198Z",
     "shell.execute_reply.started": "2025-03-06T10:51:22.900697Z"
    },
    "papermill": {
     "duration": 0.001737,
     "end_time": "2025-03-06T11:56:22.081520",
     "exception": false,
     "start_time": "2025-03-06T11:56:22.079783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1819a422",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T11:56:22.086628Z",
     "iopub.status.busy": "2025-03-06T11:56:22.086290Z",
     "iopub.status.idle": "2025-03-06T11:56:30.552654Z",
     "shell.execute_reply": "2025-03-06T11:56:30.551340Z"
    },
    "papermill": {
     "duration": 8.470909,
     "end_time": "2025-03-06T11:56:30.554371",
     "exception": false,
     "start_time": "2025-03-06T11:56:22.083462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# -------------------------\n",
    "# Custom Dataset for Spectrograms\n",
    "# -------------------------\n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.image_files[idx]\n",
    "        img_path = self.image_dir / img_file\n",
    "        image = Image.open(img_path).convert(\"L\")  # Convert to grayscale\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, img_file  # Return image tensor and filename\n",
    "\n",
    "# -------------------------\n",
    "# Define AST Model \n",
    "# -------------------------\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, overlap=6, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels=1, out_channels=embed_dim, \n",
    "                              kernel_size=patch_size, stride=patch_size-overlap)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # Shape: (batch_size, embed_dim, num_patches_h, num_patches_w)\n",
    "        x = x.flatten(2).transpose(1, 2)  # Shape: (batch_size, num_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.out = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_patches, embed_dim = x.shape\n",
    "        qkv = self.qkv(x).reshape(batch_size, num_patches, 3, self.num_heads, self.head_dim)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
    "        attn_weights = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn = F.softmax(attn_weights, dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).reshape(batch_size, num_patches, embed_dim)\n",
    "        return self.out(out)\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, embed_dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.attn(self.norm1(x)))  \n",
    "        x = x + self.dropout(self.mlp(self.norm2(x)))   \n",
    "        return x\n",
    "\n",
    "\n",
    "class AST(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, num_classes=3, embed_dim=256, \n",
    "                 num_heads=8, depth=6, mlp_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, overlap=6, embed_dim=embed_dim)\n",
    "        \n",
    "        # Calculate number of patches (H, W)\n",
    "        num_patches_h = (img_size - patch_size) // (patch_size - 6) + 1\n",
    "        num_patches_w = (img_size - patch_size) // (patch_size - 6) + 1\n",
    "        num_patches = num_patches_h * num_patches_w\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))  # Fixed here\n",
    "        \n",
    "        self.encoder = nn.Sequential(*[TransformerEncoderBlock(embed_dim, num_heads, mlp_dim, dropout) \n",
    "                                       for _ in range(depth)])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        batch_size = x.shape[0]\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        \n",
    "        # Fix position embedding size\n",
    "        x = x + self.pos_embedding[:, :x.shape[1], :]\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        x = self.norm(x[:, 0])  # CLS token output\n",
    "        \n",
    "        soft_labels = torch.sigmoid(self.head(x))  # Multi-label classification\n",
    "        return soft_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83b93932",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T11:56:30.559969Z",
     "iopub.status.busy": "2025-03-06T11:56:30.559469Z",
     "iopub.status.idle": "2025-03-06T11:56:31.204028Z",
     "shell.execute_reply": "2025-03-06T11:56:31.202862Z"
    },
    "papermill": {
     "duration": 0.648648,
     "end_time": "2025-03-06T11:56:31.205301",
     "exception": false,
     "start_time": "2025-03-06T11:56:30.556653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type:depth-idx)                        Input Shape               Output Shape              Param #\n",
       "========================================================================================================================\n",
       "AST                                           [4, 1, 224, 224]          [4, 3]                    113,408\n",
       "├─PatchEmbedding: 1-1                         [4, 1, 224, 224]          [4, 441, 256]             --\n",
       "│    └─Conv2d: 2-1                            [4, 1, 224, 224]          [4, 256, 21, 21]          65,792\n",
       "├─Sequential: 1-2                             [4, 442, 256]             [4, 442, 256]             --\n",
       "│    └─TransformerEncoderBlock: 2-2           [4, 442, 256]             [4, 442, 256]             --\n",
       "│    │    └─LayerNorm: 3-1                    [4, 442, 256]             [4, 442, 256]             512\n",
       "│    │    └─MultiHeadSelfAttention: 3-2       [4, 442, 256]             [4, 442, 256]             263,168\n",
       "│    │    └─Dropout: 3-3                      [4, 442, 256]             [4, 442, 256]             --\n",
       "│    │    └─LayerNorm: 3-4                    [4, 442, 256]             [4, 442, 256]             512\n",
       "│    │    └─Sequential: 3-5                   [4, 442, 256]             [4, 442, 256]             262,912\n",
       "│    │    └─Dropout: 3-6                      [4, 442, 256]             [4, 442, 256]             --\n",
       "│    └─TransformerEncoderBlock: 2-3           [4, 442, 256]             [4, 442, 256]             --\n",
       "│    │    └─LayerNorm: 3-7                    [4, 442, 256]             [4, 442, 256]             512\n",
       "│    │    └─MultiHeadSelfAttention: 3-8       [4, 442, 256]             [4, 442, 256]             263,168\n",
       "│    │    └─Dropout: 3-9                      [4, 442, 256]             [4, 442, 256]             --\n",
       "│    │    └─LayerNorm: 3-10                   [4, 442, 256]             [4, 442, 256]             512\n",
       "│    │    └─Sequential: 3-11                  [4, 442, 256]             [4, 442, 256]             262,912\n",
       "│    │    └─Dropout: 3-12                     [4, 442, 256]             [4, 442, 256]             --\n",
       "│    └─TransformerEncoderBlock: 2-4           [4, 442, 256]             [4, 442, 256]             --\n",
       "│    │    └─LayerNorm: 3-13                   [4, 442, 256]             [4, 442, 256]             512\n",
       "│    │    └─MultiHeadSelfAttention: 3-14      [4, 442, 256]             [4, 442, 256]             263,168\n",
       "│    │    └─Dropout: 3-15                     [4, 442, 256]             [4, 442, 256]             --\n",
       "│    │    └─LayerNorm: 3-16                   [4, 442, 256]             [4, 442, 256]             512\n",
       "│    │    └─Sequential: 3-17                  [4, 442, 256]             [4, 442, 256]             262,912\n",
       "│    │    └─Dropout: 3-18                     [4, 442, 256]             [4, 442, 256]             --\n",
       "│    └─TransformerEncoderBlock: 2-5           [4, 442, 256]             [4, 442, 256]             --\n",
       "│    │    └─LayerNorm: 3-19                   [4, 442, 256]             [4, 442, 256]             512\n",
       "│    │    └─MultiHeadSelfAttention: 3-20      [4, 442, 256]             [4, 442, 256]             263,168\n",
       "│    │    └─Dropout: 3-21                     [4, 442, 256]             [4, 442, 256]             --\n",
       "│    │    └─LayerNorm: 3-22                   [4, 442, 256]             [4, 442, 256]             512\n",
       "│    │    └─Sequential: 3-23                  [4, 442, 256]             [4, 442, 256]             262,912\n",
       "│    │    └─Dropout: 3-24                     [4, 442, 256]             [4, 442, 256]             --\n",
       "│    └─TransformerEncoderBlock: 2-6           [4, 442, 256]             [4, 442, 256]             --\n",
       "│    │    └─LayerNorm: 3-25                   [4, 442, 256]             [4, 442, 256]             512\n",
       "│    │    └─MultiHeadSelfAttention: 3-26      [4, 442, 256]             [4, 442, 256]             263,168\n",
       "│    │    └─Dropout: 3-27                     [4, 442, 256]             [4, 442, 256]             --\n",
       "│    │    └─LayerNorm: 3-28                   [4, 442, 256]             [4, 442, 256]             512\n",
       "│    │    └─Sequential: 3-29                  [4, 442, 256]             [4, 442, 256]             262,912\n",
       "│    │    └─Dropout: 3-30                     [4, 442, 256]             [4, 442, 256]             --\n",
       "│    └─TransformerEncoderBlock: 2-7           [4, 442, 256]             [4, 442, 256]             --\n",
       "│    │    └─LayerNorm: 3-31                   [4, 442, 256]             [4, 442, 256]             512\n",
       "│    │    └─MultiHeadSelfAttention: 3-32      [4, 442, 256]             [4, 442, 256]             263,168\n",
       "│    │    └─Dropout: 3-33                     [4, 442, 256]             [4, 442, 256]             --\n",
       "│    │    └─LayerNorm: 3-34                   [4, 442, 256]             [4, 442, 256]             512\n",
       "│    │    └─Sequential: 3-35                  [4, 442, 256]             [4, 442, 256]             262,912\n",
       "│    │    └─Dropout: 3-36                     [4, 442, 256]             [4, 442, 256]             --\n",
       "├─LayerNorm: 1-3                              [4, 256]                  [4, 256]                  512\n",
       "├─Linear: 1-4                                 [4, 256]                  [4, 3]                    771\n",
       "========================================================================================================================\n",
       "Total params: 3,343,107\n",
       "Trainable params: 3,343,107\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 128.71\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.80\n",
       "Forward/backward pass size (MB): 199.15\n",
       "Params size (MB): 12.92\n",
       "Estimated Total Size (MB): 212.87\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "ast_model = AST(img_size=224, patch_size=16, num_classes=3, embed_dim=256, \n",
    "                num_heads=8, depth=6, mlp_dim=512, dropout=0.1)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ast_model.to(device)\n",
    "\n",
    "summary(ast_model, input_size=(4, 1, 224, 224), col_names=[\"input_size\", \"output_size\", \"num_params\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2e27dbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T11:56:31.210843Z",
     "iopub.status.busy": "2025-03-06T11:56:31.210605Z",
     "iopub.status.idle": "2025-03-06T11:56:31.244268Z",
     "shell.execute_reply": "2025-03-06T11:56:31.243135Z"
    },
    "papermill": {
     "duration": 0.037775,
     "end_time": "2025-03-06T11:56:31.245668",
     "exception": true,
     "start_time": "2025-03-06T11:56:31.207893",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'your_dataset_directory/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1c46d3a6666f>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Create dataset and dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpectrogramDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b51005f3e05a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image_dir, transform)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".jpeg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_dataset_directory/train'"
     ]
    }
   ],
   "source": [
    "#-------------------------\n",
    "#Load Data and Use DataLoader\n",
    "#-------------------------\n",
    "data_dir = Path(\"your_dataset_directory\")  # Change this to your dataset folder\n",
    "train_dir = data_dir / \"train\"\n",
    "csv_path = data_dir / \"train_labels.csv\"\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Ensure single-channel\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "batch_size = 32 \n",
    "dataset = SpectrogramDataset(train_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "ast_model = AST()\n",
    "ast_model.eval() \n",
    "# -------------------------\n",
    "# Process Batches Using DataLoader\n",
    "# -------------------------\n",
    "df_records = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_images, batch_filenames in dataloader:\n",
    "        batch_images = batch_images.unsqueeze(1)  # Add channel dimension (B, 1, H, W)\n",
    "        \n",
    "        soft_labels = ast_model(batch_images)  # Forward pass\n",
    "        \n",
    "        for filename, label in zip(batch_filenames, soft_labels):\n",
    "            df_records.append([filename] + label.tolist())\n",
    "\n",
    "# -------------------------\n",
    "# Save Predictions to CSV\n",
    "# -------------------------\n",
    "class_names = [\"Class_1\", \"Class_2\", \"Class_3\"]  # Modify with actual class names\n",
    "df = pd.DataFrame(df_records, columns=[\"Path\"] + class_names)\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Soft labels generated! Labels saved in {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18.995726,
   "end_time": "2025-03-06T11:56:32.972220",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-06T11:56:13.976494",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
