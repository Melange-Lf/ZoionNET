{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# sys.path.append(os.path.dirname(__file__))\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 6/10/21 5:04 PM\n",
    "# @Author  : Yuan Gong\n",
    "# @Affiliation  : Massachusetts Institute of Technology\n",
    "# @Email   : yuangong@mit.edu\n",
    "# @File    : ast_models.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast\n",
    "import os\n",
    "import wget\n",
    "os.environ['TORCH_HOME'] = '../pretrained_models'\n",
    "import timm\n",
    "from timm.models.layers import to_2tuple,trunc_normal_\n",
    "\n",
    "# override the timm package to relax the input shape constraint.\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class ASTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The AST model.\n",
    "    :param label_dim: the label dimension, i.e., the number of total classes, it is 527 for AudioSet, 50 for ESC-50, and 35 for speechcommands v2-35\n",
    "    :param fstride: the stride of patch spliting on the frequency dimension, for 16*16 patchs, fstride=16 means no overlap, fstride=10 means overlap of 6\n",
    "    :param tstride: the stride of patch spliting on the time dimension, for 16*16 patchs, tstride=16 means no overlap, tstride=10 means overlap of 6\n",
    "    :param input_fdim: the number of frequency bins of the input spectrogram\n",
    "    :param input_tdim: the number of time frames of the input spectrogram\n",
    "    :param imagenet_pretrain: if use ImageNet pretrained model\n",
    "    :param audioset_pretrain: if use full AudioSet and ImageNet pretrained model\n",
    "    :param model_size: the model size of AST, should be in [tiny224, small224, base224, base384], base224 and base 384 are same model, but are trained differently during ImageNet pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self, label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=True, audioset_pretrain=False, model_size='base384', verbose=True):\n",
    "\n",
    "        super(ASTModel, self).__init__()\n",
    "\n",
    "        if verbose == True:\n",
    "            print('---------------AST Model Summary---------------')\n",
    "            print('ImageNet pretraining: {:s}, AudioSet pretraining: {:s}'.format(str(imagenet_pretrain),str(audioset_pretrain)))\n",
    "        # override timm input shape restriction\n",
    "        timm.models.vision_transformer.PatchEmbed = PatchEmbed\n",
    "\n",
    "        # if AudioSet pretraining is not used (but ImageNet pretraining may still apply)\n",
    "        if audioset_pretrain == False:\n",
    "            if model_size == 'tiny224':\n",
    "                self.v = timm.create_model('vit_deit_tiny_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
    "            elif model_size == 'small224':\n",
    "                self.v = timm.create_model('vit_deit_small_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
    "            elif model_size == 'base224':\n",
    "                self.v = timm.create_model('vit_deit_base_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
    "            elif model_size == 'base384':\n",
    "                self.v = timm.create_model('vit_deit_base_distilled_patch16_384', pretrained=imagenet_pretrain)\n",
    "            else:\n",
    "                raise Exception('Model size must be one of tiny224, small224, base224, base384.')\n",
    "            self.original_num_patches = self.v.patch_embed.num_patches\n",
    "            self.oringal_hw = int(self.original_num_patches ** 0.5)\n",
    "            self.original_embedding_dim = self.v.pos_embed.shape[2]\n",
    "            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n",
    "\n",
    "            # automatcially get the intermediate shape\n",
    "            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n",
    "            num_patches = f_dim * t_dim\n",
    "            self.v.patch_embed.num_patches = num_patches\n",
    "            if verbose == True:\n",
    "                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n",
    "                print('number of patches={:d}'.format(num_patches))\n",
    "\n",
    "            # the linear projection layer\n",
    "            new_proj = torch.nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n",
    "            if imagenet_pretrain == True:\n",
    "                new_proj.weight = torch.nn.Parameter(torch.sum(self.v.patch_embed.proj.weight, dim=1).unsqueeze(1))\n",
    "                new_proj.bias = self.v.patch_embed.proj.bias\n",
    "            self.v.patch_embed.proj = new_proj\n",
    "\n",
    "            # the positional embedding\n",
    "            if imagenet_pretrain == True:\n",
    "                # get the positional embedding from deit model, skip the first two tokens (cls token and distillation token), reshape it to original 2D shape (24*24).\n",
    "                new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, self.original_num_patches, self.original_embedding_dim).transpose(1, 2).reshape(1, self.original_embedding_dim, self.oringal_hw, self.oringal_hw)\n",
    "                # cut (from middle) or interpolate the second dimension of the positional embedding\n",
    "                if t_dim <= self.oringal_hw:\n",
    "                    new_pos_embed = new_pos_embed[:, :, :, int(self.oringal_hw / 2) - int(t_dim / 2): int(self.oringal_hw / 2) - int(t_dim / 2) + t_dim]\n",
    "                else:\n",
    "                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(self.oringal_hw, t_dim), mode='bilinear')\n",
    "                # cut (from middle) or interpolate the first dimension of the positional embedding\n",
    "                if f_dim <= self.oringal_hw:\n",
    "                    new_pos_embed = new_pos_embed[:, :, int(self.oringal_hw / 2) - int(f_dim / 2): int(self.oringal_hw / 2) - int(f_dim / 2) + f_dim, :]\n",
    "                else:\n",
    "                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(f_dim, t_dim), mode='bilinear')\n",
    "                # flatten the positional embedding\n",
    "                new_pos_embed = new_pos_embed.reshape(1, self.original_embedding_dim, num_patches).transpose(1,2)\n",
    "                # concatenate the above positional embedding with the cls token and distillation token of the deit model.\n",
    "                self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n",
    "            else:\n",
    "                # if not use imagenet pretrained model, just randomly initialize a learnable positional embedding\n",
    "                # TODO can use sinusoidal positional embedding instead\n",
    "                new_pos_embed = nn.Parameter(torch.zeros(1, self.v.patch_embed.num_patches + 2, self.original_embedding_dim))\n",
    "                self.v.pos_embed = new_pos_embed\n",
    "                trunc_normal_(self.v.pos_embed, std=.02)\n",
    "\n",
    "        # now load a model that is pretrained on both ImageNet and AudioSet\n",
    "        elif audioset_pretrain == True:\n",
    "            if audioset_pretrain == True and imagenet_pretrain == False:\n",
    "                raise ValueError('currently model pretrained on only audioset is not supported, please set imagenet_pretrain = True to use audioset pretrained model.')\n",
    "            if model_size != 'base384':\n",
    "                raise ValueError('currently only has base384 AudioSet pretrained model.')\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            if os.path.exists('../pretrained_models/audioset_10_10_0.4593.pth') == False:\n",
    "                # this model performs 0.4593 mAP on the audioset eval set\n",
    "                audioset_mdl_url = 'https://www.dropbox.com/s/cv4knew8mvbrnvq/audioset_0.4593.pth?dl=1'\n",
    "                wget.download(audioset_mdl_url, out='../pretrained_models/audioset_10_10_0.4593.pth')\n",
    "            sd = torch.load('../pretrained_models/audioset_10_10_0.4593.pth', map_location=device)\n",
    "            audio_model = ASTModel(label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=False, audioset_pretrain=False, model_size='base384', verbose=False)\n",
    "            audio_model = torch.nn.DataParallel(audio_model)\n",
    "            audio_model.load_state_dict(sd, strict=False)\n",
    "            self.v = audio_model.module.v\n",
    "            self.original_embedding_dim = self.v.pos_embed.shape[2]\n",
    "            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n",
    "\n",
    "            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n",
    "            num_patches = f_dim * t_dim\n",
    "            self.v.patch_embed.num_patches = num_patches\n",
    "            if verbose == True:\n",
    "                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n",
    "                print('number of patches={:d}'.format(num_patches))\n",
    "\n",
    "            new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, 1212, 768).transpose(1, 2).reshape(1, 768, 12, 101)\n",
    "            # if the input sequence length is larger than the original audioset (10s), then cut the positional embedding\n",
    "            if t_dim < 101:\n",
    "                new_pos_embed = new_pos_embed[:, :, :, 50 - int(t_dim/2): 50 - int(t_dim/2) + t_dim]\n",
    "            # otherwise interpolate\n",
    "            else:\n",
    "                new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(12, t_dim), mode='bilinear')\n",
    "            if f_dim < 12:\n",
    "                new_pos_embed = new_pos_embed[:, :, 6 - int(f_dim/2): 6 - int(f_dim/2) + f_dim, :]\n",
    "            # otherwise interpolate\n",
    "            elif f_dim > 12:\n",
    "                new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(f_dim, t_dim), mode='bilinear')\n",
    "            new_pos_embed = new_pos_embed.reshape(1, 768, num_patches).transpose(1, 2)\n",
    "            self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n",
    "\n",
    "    def get_shape(self, fstride, tstride, input_fdim=128, input_tdim=1024):\n",
    "        test_input = torch.randn(1, 1, input_fdim, input_tdim)\n",
    "        test_proj = nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n",
    "        test_out = test_proj(test_input)\n",
    "        f_dim = test_out.shape[2]\n",
    "        t_dim = test_out.shape[3]\n",
    "        return f_dim, t_dim\n",
    "\n",
    "    # @autocast()\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: the input spectrogram, expected shape: (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
    "        :return: prediction\n",
    "        \"\"\"\n",
    "        # expect input x = (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        # Convert input to the same dtype as the model's parameters\n",
    "        # x = x.to(next(self.parameters()).dtype)\n",
    "\n",
    "        B = x.shape[0]\n",
    "        print(\"path embed\")\n",
    "        x = self.v.patch_embed(x)\n",
    "        cls_tokens = self.v.cls_token.expand(B, -1, -1)\n",
    "        dist_token = self.v.dist_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
    "        x = x + self.v.pos_embed\n",
    "        x = self.v.pos_drop(x)\n",
    "        for blk in self.v.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.v.norm(x)\n",
    "        x = (x[:, 0] + x[:, 1]) / 2\n",
    "\n",
    "        x = self.mlp_head(x)\n",
    "        return x\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     input_tdim = 100\n",
    "#     ast_mdl = ASTModel(input_tdim=input_tdim)\n",
    "#     # input a batch of 10 spectrogram, each with 100 time frames and 128 frequency bins\n",
    "\n",
    "#     device = 'cpu'\n",
    "\n",
    "#     # 1) cast to float, 2) move model to GPU\n",
    "#     ast_mdl = ast_mdl.float().to(device)\n",
    "\n",
    "#     # 3) create your input and move it to the *same* device\n",
    "#     test_input = torch.rand([10, input_tdim, 128], device=device)\n",
    "#     test_output = ast_mdl(test_input)\n",
    "#     # output should be in shape [10, 527], i.e., 10 samples, each with prediction of 527 classes.\n",
    "#     print(test_output.shape)\n",
    "\n",
    "#     input_tdim = 256\n",
    "#     ast_mdl = ASTModel(input_tdim=input_tdim,label_dim=50, audioset_pretrain=True)\n",
    "#     # input a batch of 10 spectrogram, each with 512 time frames and 128 frequency bins\n",
    "#     test_input = torch.rand([10, input_tdim, 128])\n",
    "#     test_output = ast_mdl(test_input)\n",
    "#     # output should be in shape [10, 50], i.e., 10 samples, each with prediction of 50 classes.\n",
    "#     print(test_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "import csv\n",
    "import time\n",
    "from torch import nn\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def load_config(config_path):\n",
    "    \"\"\"Load the configuration file containing preprocessing parameters.\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    return config\n",
    "\n",
    "class FbankDataset(Dataset):\n",
    "    \"\"\"Dataset class for preprocessed fbank files.\n",
    "    \n",
    "    This dataset class works with the preprocessed fbank files generated by process_dataset.\n",
    "    It loads the preprocessed mel spectrograms directly from disk instead of computing them on the fly.\n",
    "    \n",
    "    Args:\n",
    "        dataset_json_file (str): Path to the JSON file containing fbank file paths and labels\n",
    "        label_csv (str, optional): Path to the CSV file containing class labels\n",
    "        audio_conf (dict, optional): Dictionary containing audio configuration parameters\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_json_file, label_csv=None, audio_conf=None):\n",
    "        self.datapath = dataset_json_file\n",
    "        with open(dataset_json_file, 'r') as fp:\n",
    "            data_json = json.load(fp)\n",
    "        \n",
    "        self.data = data_json['data']\n",
    "        self.audio_conf = audio_conf or {}\n",
    "        \n",
    "        # Load label mapping if provided\n",
    "        self.index_dict = {}\n",
    "        if label_csv:\n",
    "            with open(label_csv, 'r') as f:\n",
    "                csv_reader = csv.DictReader(f)\n",
    "                for row in csv_reader:\n",
    "                    self.index_dict[row['mid']] = row['index']\n",
    "            self.label_num = len(self.index_dict)\n",
    "        else:\n",
    "            self.label_num = None\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Get a single item from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            index (int): Index of the item to get\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (fbank, label_indices)\n",
    "                - fbank (torch.Tensor): The preprocessed mel spectrogram\n",
    "                - label_indices (torch.Tensor): The label indices\n",
    "        \"\"\"\n",
    "        datum = self.data[index]\n",
    "        \n",
    "        # Load the preprocessed fbank\n",
    "        fbank = torch.load(datum['wav'])\n",
    "        \n",
    "        # Initialize label indices\n",
    "        if self.label_num is not None:\n",
    "            label_indices = np.zeros(self.label_num)\n",
    "            for label_str in datum['labels'].split(','):\n",
    "                label_indices[int(self.index_dict[label_str])] = 1.0\n",
    "            label_indices = torch.FloatTensor(label_indices)\n",
    "        else:\n",
    "            label_indices = torch.tensor(0)  # Dummy tensor if no labels\n",
    "        \n",
    "        return fbank, label_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Get the total number of items in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device, epoch, args):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    loss_meter = AverageMeter()\n",
    "    \n",
    "\n",
    "    print(\"initialized time measures\")\n",
    "    end = time.time()\n",
    "    print(\"unrolling loader\")\n",
    "    for i, (audio_input, labels) in enumerate(train_loader):\n",
    "        print(\"updating time\")\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        print(\"moving input to device\")\n",
    "        # Move data to device\n",
    "        audio_input = audio_input.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        print(\"train forw\")\n",
    "        # Compute output\n",
    "        audio_output = model(audio_input)\n",
    "        \n",
    "        # Compute loss\n",
    "        if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "            loss = criterion(audio_output, torch.argmax(labels.long(), axis=1))\n",
    "        else:\n",
    "            loss = criterion(audio_output, labels)\n",
    "        \n",
    "        # Compute gradient and do optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update statistics\n",
    "        loss_meter.update(loss.item(), audio_input.size(0))\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        # Print statistics\n",
    "        if i % args['print_freq'] == 0:\n",
    "            print(f'Epoch: [{epoch}][{i}/{len(train_loader)}]\\t'\n",
    "                  f'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  f'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  f'Loss {loss_meter.val:.4f} ({loss_meter.avg:.4f})')\n",
    "    \n",
    "    return loss_meter.avg\n",
    "\n",
    "def validate(model, val_loader, criterion, device, args):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    batch_time = AverageMeter()\n",
    "    loss_meter = AverageMeter()\n",
    "    \n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (audio_input, labels) in enumerate(val_loader):\n",
    "            # Move data to device\n",
    "            audio_input = audio_input.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Compute output\n",
    "            audio_output = model(audio_input)\n",
    "            \n",
    "            # Compute loss\n",
    "            if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "                loss = criterion(audio_output, torch.argmax(labels.long(), axis=1))\n",
    "            else:\n",
    "                loss = criterion(audio_output, labels)\n",
    "            \n",
    "            # Store predictions and targets\n",
    "            predictions.append(audio_output.cpu())\n",
    "            targets.append(labels.cpu())\n",
    "            \n",
    "            # Update statistics\n",
    "            loss_meter.update(loss.item(), audio_input.size(0))\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            \n",
    "            if i % args['print_freq'] == 0:\n",
    "                print(f'Validation: [{i}/{len(val_loader)}]\\t'\n",
    "                      f'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      f'Loss {loss_meter.val:.4f} ({loss_meter.avg:.4f})')\n",
    "    \n",
    "    # Concatenate predictions and targets\n",
    "    predictions = torch.cat(predictions)\n",
    "    targets = torch.cat(targets)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "        accuracy = (torch.argmax(predictions, dim=1) == torch.argmax(targets, dim=1)).float().mean()\n",
    "        metrics = {'accuracy': accuracy.item(), 'loss': loss_meter.avg}\n",
    "    else:\n",
    "        # For binary/multi-label classification\n",
    "        predictions = torch.sigmoid(predictions)\n",
    "        metrics = {\n",
    "            'loss': loss_meter.avg,\n",
    "            'accuracy': ((predictions > 0.5) == targets).float().mean().item()\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def train_model(model, train_loader, val_loader, args):\n",
    "    \"\"\"Main training function.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'Using device: {device}')\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    # Define loss function\n",
    "    if args['loss'] == 'BCE':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    elif args['loss'] == 'CE':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        raise ValueError(f'Unknown loss function: {args[\"loss\"]}')\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "    \n",
    "    # Define learning rate scheduler\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    #     optimizer, mode='min', factor=0.5, patience=args.lr_patience, verbose=True\n",
    "    # )\n",
    "\n",
    "    # ideally for ESC\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, list(range(args['lrscheduler_start'], 1000, args['lrscheduler_step'])),gamma=args['lrscheduler_decay'])\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(args['epochs']):\n",
    "        print(f'\\nEpoch {epoch+1}/{args[\"epochs\"]}')\n",
    "        \n",
    "        # Train for one epoch\n",
    "        print(\"train loop start\")\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device, epoch, args)\n",
    "        \n",
    "        # Validate\n",
    "        val_metrics = validate(model, val_loader, criterion, device, args)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_metrics['loss'])\n",
    "        \n",
    "        # Save checkpoint\n",
    "        is_best = val_metrics['loss'] < best_val_loss\n",
    "        best_val_loss = min(val_metrics['loss'], best_val_loss)\n",
    "        \n",
    "        if is_best:\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_metrics['loss'],\n",
    "                'val_accuracy': val_metrics['accuracy']\n",
    "            }, os.path.join(args['exp_dir'], 'best_model.pth'))\n",
    "        \n",
    "        # Save latest checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'val_accuracy': val_metrics['accuracy']\n",
    "        }, os.path.join(args['exp_dir'], 'latest_model.pth'))\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}')\n",
    "        print(f'Val Loss: {val_metrics[\"loss\"]:.4f}')\n",
    "        print(f'Val Accuracy: {val_metrics[\"accuracy\"]:.4f}')\n",
    "\n",
    "def run_inference(model, val_json_path, label_csv_path, output_csv_path, device=None):\n",
    "    \"\"\"Run inference on a validation set and save predictions to CSV.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained model\n",
    "        val_json_path (str): Path to the validation JSON file containing fbank file paths\n",
    "        label_csv_path (str): Path to the CSV file containing class labels\n",
    "        output_csv_path (str): Path where the output CSV file will be saved\n",
    "        device (torch.device, optional): Device to run inference on. If None, will use CUDA if available.\n",
    "    \n",
    "    Returns:\n",
    "        float: Overall accuracy of the model on the validation set\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load validation data\n",
    "    with open(val_json_path, 'r') as f:\n",
    "        val_data = json.load(f)\n",
    "    \n",
    "    # Load label mapping\n",
    "    label_map = {}\n",
    "    with open(label_csv_path, 'r') as f:\n",
    "        csv_reader = csv.DictReader(f)\n",
    "        for row in csv_reader:\n",
    "            label_map[row['index']] = row['mid']\n",
    "    num_classes = len(label_map)\n",
    "    \n",
    "    # Prepare output CSV\n",
    "    output_columns = ['file_path'] + [f'prob_class_{i}' for i in range(num_classes)] + ['true_label', 'predicted_label', 'correct']\n",
    "    output_data = []\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Process each file\n",
    "    for item in tqdm(val_data['data'], desc=\"Running inference\"):\n",
    "        # Load fbank\n",
    "        fbank = torch.load(item['wav'])\n",
    "        fbank = fbank.unsqueeze(0).to(device)  # Add batch dimension\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = item['labels'].split(',')[0]  # Assuming single label per file\n",
    "        \n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            logits = model(fbank)\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "        \n",
    "        # Get predicted class\n",
    "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "        predicted_label = label_map[str(predicted_class)]\n",
    "        \n",
    "        # Check if prediction is correct\n",
    "        is_correct = predicted_label == true_label\n",
    "        if is_correct:\n",
    "            correct_predictions += 1\n",
    "        total_predictions += 1\n",
    "        \n",
    "        # Prepare row for CSV\n",
    "        row = [item['wav']]  # File path\n",
    "        row.extend(probabilities[0].cpu().numpy())  # Probabilities for each class\n",
    "        row.extend([true_label, predicted_label, is_correct])\n",
    "        output_data.append(row)\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    # Save results to CSV\n",
    "    df = pd.DataFrame(output_data, columns=output_columns)\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    print(f\"\\nInference complete!\")\n",
    "    print(f\"Overall accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Results saved to: {output_csv_path}\")\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration dictionary\n",
    "config_dict = {\n",
    "    'num_mel_bins': 128,\n",
    "    'target_length': 512, # {'audioset':1024, 'esc50':512, 'speechcommands':128}\n",
    "    'loss' : 'CE',\n",
    "    'mode':'train', \n",
    "    'mean':-6.6268077, # ESC -6.6268077\n",
    "    'std' : 5.358466, # ESC 5.358466\n",
    "    'fstride' : 10,\n",
    "    'tstride' : 10,\n",
    "    'input_fdim' : 128,\n",
    "    'input_tdim' : 512,\n",
    "    'imagenet_pretrain' : True,\n",
    "    'audioset_pretrain' : True,\n",
    "    'model_size' : 'base384',\n",
    "    'epochs' : 25,\n",
    "    'lr' : 1e-5, # audioset pretrain is false, then one order up\n",
    "    'weight_decay' : 5e-7,\n",
    "    'betas' : (0.95, 0.999),\n",
    "    'lrscheduler_start' : 5,\n",
    "    'lrscheduler_step' : 1,\n",
    "    'lrscheduler_decay' : 0.85,\n",
    "    'print_freq' : 0,\n",
    "    'exp_dir' : \"AST/ast-master/egs/esc50/exp/custom_run\"\n",
    "}\n",
    "\n",
    "# Paths\n",
    "train_json = \"../egs/esc50/data/datafiles_fbank/esc_train_data_1.json\"\n",
    "eval_json = \"../egs/esc50/data/datafiles_fbank/esc_eval_data_1.json\"\n",
    "label_csv = \"../egs/esc50/data/esc_class_labels_indices.csv\"\n",
    "train_out_csv = \"../egs/esc50/exp/custom_run/esc_train_1.csv\"\n",
    "eval_out_csv = \"../egs/esc50/exp/custom_run/esc_eval_1.csv\"\n",
    "os.makedirs(config_dict['exp_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "train_dataset = FbankDataset(train_json, label_csv=label_csv)\n",
    "eval_dataset = FbankDataset(eval_json, label_csv=label_csv)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True, num_workers=8, pin_memory=True\n",
    ")\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "    eval_dataset, batch_size=64, shuffle=False, num_workers=8, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outside = 0\n",
    "for item in train_loader:\n",
    "    outside = item\n",
    "    break\n",
    "\n",
    "outside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------AST Model Summary---------------\n",
      "ImageNet pretraining: True, AudioSet pretraining: True\n",
      "frequncey stride=10, time stride=10\n",
      "number of patches=600\n"
     ]
    }
   ],
   "source": [
    "model = ASTModel(\n",
    "    label_dim=len(train_dataset.index_dict),\n",
    "    fstride=config_dict['fstride'],\n",
    "    tstride=config_dict['tstride'],\n",
    "    input_fdim=config_dict['input_fdim'],\n",
    "    input_tdim=config_dict['input_tdim'],\n",
    "    imagenet_pretrain=config_dict['imagenet_pretrain'],\n",
    "    audioset_pretrain=config_dict['audioset_pretrain'],\n",
    "    model_size=config_dict['model_size']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Epoch 1/25\n",
      "train loop start\n",
      "initialized time measures\n",
      "unrolling loader\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, eval_loader, config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(model, train_json, label_csv, train_out_csv)\n",
    "run_inference(model, eval_json, label_csv, eval_out_csv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
