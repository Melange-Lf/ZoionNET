{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\Py\\Ai\\IEEE comsoc\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, ASTForAudioClassification, ASTConfig, ASTModel\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset, DownloadConfig\n",
    "import combined_utils\n",
    "import os\n",
    "from outsource import AudioSetAST, create_label_mapping, collate_fn, create_multihot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../..\")\n",
    "# os.getcwd()\n",
    "\n",
    "# All paths now need to be relative to the project dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crnn.code import engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASetFineAnyAST(nn.Module):\n",
    "\n",
    "    def __init__(self, inp_t=512, inp_f=128, n_classes=50):\n",
    "        super(ASetFineAnyAST, self).__init__()\n",
    "\n",
    "        self.backbone = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "        # Note that the embedding dim is 768 from the pretrained model\n",
    "\n",
    "\n",
    "        # Replacing the old head for the new classes\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=(768,), eps=1e-12, elementwise_affine=True),\n",
    "            nn.Linear(in_features=768, out_features=n_classes, bias=True)\n",
    "            )\n",
    "        # Modifying the positional embeddings\n",
    "\n",
    "        H_old = (1024 - 16) // 10 + 1\n",
    "        W_old = (128  - 16) // 10 + 1\n",
    "        H_new = (inp_t - 16) // 10 + 1\n",
    "        W_new = (inp_f  - 16) // 10 + 1\n",
    "        \n",
    "        N_old = H_old * W_old\n",
    "        N_new = H_new * W_new\n",
    "\n",
    "        old_pos = self.backbone.audio_spectrogram_transformer.embeddings.position_embeddings.data\n",
    "        \n",
    "        # (CLS-pos, patch-pos grid, DIST-pos):\n",
    "        cls_pe   = old_pos[:, 0, :]              \n",
    "        patch_pe = old_pos[:, 1 : 1 + N_old, :]   \n",
    "        dist_pe  = old_pos[:, 1 +N_old , :]      \n",
    "\n",
    "\n",
    "        # Reshaping patch_pe  [1, D, H_old, W_old] for interpolation to [1, D, H_new, W_new]:\n",
    "        D = patch_pe.size(-1)\n",
    "        patch_pe = patch_pe.view(1, H_old, W_old, D).permute(0, 3, 1, 2)  # [1, D, H_old, W_old]\n",
    "\n",
    "        patch_pe = F.interpolate(\n",
    "            patch_pe,\n",
    "            size=(H_new, W_new),\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "        patch_pe = patch_pe.permute(0, 2, 3, 1).reshape(1, N_new, D)     # [1, N_new, D]\n",
    "\n",
    "        # print(cls_pe.shape, patch_pe.shape, dist_pe.shape)\n",
    "        # Concat back CLS-pos and DIST-pos\n",
    "        new_pos = torch.cat([cls_pe.unsqueeze_(0), patch_pe, dist_pe.unsqueeze_(0)], dim=1)         # [1, 1+N_new+1, D]\n",
    "\n",
    "\n",
    "        # Overwrite the original embeddings\n",
    "        self.backbone.audio_spectrogram_transformer.embeddings.position_embeddings = nn.Parameter(new_pos, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "    \n",
    "        return self.backbone.forward(x).logits\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wav_dir = \"AST/ast-master/egs/esc50/data/ESC-50-master/audio\"\n",
    "# config_path = \"AST/ast-master/egs/esc50/data/datafiles/esc50_config.json\"\n",
    "\n",
    "# Configuration dictionary\n",
    "config_dict = {'num_mel_bins': 128,\n",
    "            'target_length': 512, # {'audioset':1024, 'esc50':512, 'speechcommands':128}\n",
    "            'loss' : 'CE',\n",
    "            'mode':'train', \n",
    "            'mean':-6.6268077, # ESC -6.6268077, AUDIOSET -4.2677393\n",
    "            'std' : 5.358466, # ESC 5.358466, AUDIOSET 4.5689974\n",
    "            'fstride' : 10,\n",
    "            'tstride' : 10,\n",
    "            'input_fdim' : 128,\n",
    "            'input_tdim' : 512,\n",
    "            'imagenet_pretrain' : True,\n",
    "            'audioset_pretrain' : True,\n",
    "            'model_size' : 'base384',\n",
    "            'epochs' : 5,\n",
    "            'lr' : 1e-5, # if audioset pretrain is false, then value one order up (1e-4)\n",
    "            'weight_decay' : 5e-7,\n",
    "            'betas' : (0.95, 0.999),\n",
    "            'lrscheduler_start' : 5,\n",
    "            'lrscheduler_step' : 1,\n",
    "            'lrscheduler_decay' : 0.85,\n",
    "            'print_freq' : 100,\n",
    "            'exp_dir' : \"./exp/landing\"\n",
    "}\n",
    "\n",
    "# Paths\n",
    "train_json = \"AST/finetuned/data/datafiles_fbank/esc_train_data_1.json\"\n",
    "eval_json = \"AST/finetuned/data/datafiles_fbank/esc_eval_data_1.json\"\n",
    "label_csv = \"AST/finetuned/data/esc_class_labels_indices.csv\"\n",
    "\n",
    "# Dataloaders\n",
    "train_dataset = combined_utils.FbankDataset(train_json, label_csv=label_csv)\n",
    "eval_dataset = combined_utils.FbankDataset(eval_json, label_csv=label_csv)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True, num_workers=8, pin_memory=True\n",
    ")\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "    eval_dataset, batch_size=64, shuffle=False, num_workers=8, pin_memory=True\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device\n",
    "# temp_train_dataset = combined.FbankDataset(train_json_path, csv_path, config_dict)\n",
    "\n",
    "# os.makedirs(config_dict['exp_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ASetFineAnyAST(inp_t=512, inp_f=128, n_classes=50)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), config_dict['lr'], weight_decay=config_dict['weight_decay'], betas=config_dict['betas'])\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# accuracy_multi_class = lambda pred, tar: (torch.argmax(pred, dim=1) == torch.argmax(tar, dim=1)).float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Epoch 1/5\n",
      "Initiating training batches\n",
      "Epoch: [0][0/400]\tTime 0.543 (0.543)\tData 0.009 (0.009)\tLoss 4.2201 (4.2201)\n",
      "Epoch: [0][100/400]\tTime 0.634 (0.625)\tData 0.010 (0.013)\tLoss 2.1011 (3.7279)\n",
      "Epoch: [0][200/400]\tTime 0.629 (0.633)\tData 0.009 (0.013)\tLoss 1.3429 (3.0048)\n",
      "Epoch: [0][300/400]\tTime 0.336 (0.579)\tData 0.016 (0.013)\tLoss 2.1317 (2.4411)\n",
      "Validation: [0/100]\tTime 0.137 (0.137)\tLoss 1.0083 (1.0083)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\Py\\Ai\\IEEE comsoc\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.0757\n",
      "Val Loss: 0.6812\n",
      "Val Accuracy: 0.9100\n",
      "\n",
      "Epoch 2/5\n",
      "Initiating training batches\n",
      "Epoch: [1][0/400]\tTime 0.696 (0.696)\tData 0.000 (0.000)\tLoss 0.2690 (0.2690)\n",
      "Epoch: [1][100/400]\tTime 0.384 (0.400)\tData 0.032 (0.024)\tLoss 0.1021 (0.4951)\n",
      "Epoch: [1][200/400]\tTime 0.408 (0.407)\tData 0.034 (0.028)\tLoss 0.0991 (0.4043)\n",
      "Epoch: [1][300/400]\tTime 0.946 (0.478)\tData 0.013 (0.026)\tLoss 0.3112 (0.3390)\n",
      "Validation: [0/100]\tTime 0.135 (0.135)\tLoss 0.2563 (0.2563)\n",
      "Train Loss: 0.3097\n",
      "Val Loss: 0.3435\n",
      "Val Accuracy: 0.9350\n",
      "\n",
      "Epoch 3/5\n",
      "Initiating training batches\n",
      "Epoch: [2][0/400]\tTime 0.509 (0.509)\tData 0.000 (0.000)\tLoss 0.0640 (0.0640)\n",
      "Epoch: [2][100/400]\tTime 0.441 (0.398)\tData 0.038 (0.023)\tLoss 0.0394 (0.1084)\n",
      "Epoch: [2][200/400]\tTime 0.931 (0.485)\tData 0.012 (0.024)\tLoss 0.0399 (0.0980)\n",
      "Epoch: [2][300/400]\tTime 0.486 (0.626)\tData 0.002 (0.019)\tLoss 0.0532 (0.0814)\n",
      "Validation: [0/100]\tTime 0.161 (0.161)\tLoss 0.4465 (0.4465)\n",
      "Train Loss: 0.0751\n",
      "Val Loss: 0.2575\n",
      "Val Accuracy: 0.9450\n",
      "\n",
      "Epoch 4/5\n",
      "Initiating training batches\n",
      "Epoch: [3][0/400]\tTime 0.675 (0.675)\tData 0.014 (0.014)\tLoss 0.0244 (0.0244)\n",
      "Epoch: [3][100/400]\tTime 0.957 (0.594)\tData 0.000 (0.022)\tLoss 0.0188 (0.0349)\n",
      "Epoch: [3][200/400]\tTime 0.938 (0.770)\tData 0.003 (0.015)\tLoss 0.0220 (0.0371)\n",
      "Epoch: [3][300/400]\tTime 0.410 (0.724)\tData 0.025 (0.014)\tLoss 0.0339 (0.0326)\n",
      "Validation: [0/100]\tTime 0.157 (0.157)\tLoss 0.1799 (0.1799)\n",
      "Train Loss: 0.0304\n",
      "Val Loss: 0.2275\n",
      "Val Accuracy: 0.9550\n",
      "\n",
      "Epoch 5/5\n",
      "Initiating training batches\n",
      "Epoch: [4][0/400]\tTime 0.865 (0.865)\tData 0.000 (0.000)\tLoss 0.0137 (0.0137)\n",
      "Epoch: [4][100/400]\tTime 0.931 (0.947)\tData 0.010 (0.008)\tLoss 0.0121 (0.0180)\n",
      "Epoch: [4][200/400]\tTime 0.943 (0.949)\tData 0.000 (0.008)\tLoss 0.0131 (0.0172)\n",
      "Epoch: [4][300/400]\tTime 0.445 (0.830)\tData 0.026 (0.009)\tLoss 0.0208 (0.0161)\n",
      "Validation: [0/100]\tTime 0.317 (0.317)\tLoss 0.0391 (0.0391)\n",
      "Train Loss: 0.0157\n",
      "Val Loss: 0.2113\n",
      "Val Accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "os.makedirs(config_dict['exp_dir'], exist_ok=True)\n",
    "combined_utils.train_model(model, train_loader, eval_loader, config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = torch.load(\"exp/best_5epoch.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(check['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference: 100%|██████████| 1600/1600 [01:00<00:00, 26.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference complete!\n",
      "Overall accuracy: 1.0000\n",
      "Results saved to: exp/landing/fold1_train_soft_labels.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generating soft labels\n",
    "combined_utils.run_inference(model, train_json, label_csv,'exp/landing/fold1_train_soft_labels.csv', 'cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_utils.run_inference(model, eval_json, label_csv,'exp/landing/fold1_eval_soft_labels.csv', 'cuda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
