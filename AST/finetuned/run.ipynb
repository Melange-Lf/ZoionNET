{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\Py\\Ai\\IEEE comsoc\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, ASTForAudioClassification, ASTConfig, ASTModel\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import AST.finetuned.combined_utils as combined_utils\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING AREA and new model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-7.1000) Speech\n",
      "0.17\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\", trust_remote_code=True)\n",
    "# dataset = dataset.sort(\"id\")\n",
    "# sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "\n",
    "# feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "# TESTModel = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "\n",
    "# # audio file is decoded on the fly\n",
    "# inputs = feature_extractor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     logits = TESTModel(**inputs).logits\n",
    "\n",
    "# predicted_class_ids = torch.argmax(logits, dim=-1).item()\n",
    "# predicted_label = TESTModel.config.id2label[predicted_class_ids]\n",
    "# print(logits[0, 42], predicted_label)\n",
    "\n",
    "# # compute loss - target_label is e.g. \"down\"\n",
    "# target_label = TESTModel.config.id2label[0]\n",
    "# inputs[\"labels\"] = torch.tensor([TESTModel.config.label2id[target_label]])\n",
    "# loss = TESTModel(**inputs).loss\n",
    "# print(round(loss.item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768]) torch.Size([1, 600, 768]) torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "testModel = ASetFineAnyAST()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(testModel.backbone, col_names=[\"input_size\", \"output_size\", \"num_params\"], input_size=(1, 512, 128))\n",
    "# summary(model, col_names=[\"input_size\", \"output_size\", \"num_params\"], input_size=(4, 1024, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration = ASTConfig()\n",
    "\n",
    "# configuration.max_length = 1024\n",
    "# # Initializing a model (with random weights) from the MIT/ast-finetuned-audioset-10-10-0.4593 style configuration\n",
    "# modeltest = ASTModel(configuration)\n",
    "\n",
    "# # Accessing the model configuration\n",
    "# modeltest.config\n",
    "\n",
    "\n",
    "# inputs = feature_extractor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     logits = modeltest(**inputs)\n",
    "# # logits.last_hidden_state.shape, logits.pooler_output.shape\n",
    "\n",
    "# summary(modeltest, verbose=True, col_names=[\"input_size\", \"output_size\", \"num_params\"], input_size=(1, 1024, 128))\n",
    "# predicted_class_ids = torch.argmax(logits, dim=-1).item()\n",
    "# predicted_label = modeltest.config.id2label[predicted_class_ids]\n",
    "# print(logits[42], predicted_label)\n",
    "\n",
    "# # compute loss - target_label is e.g. \"down\"\n",
    "# target_label = modeltest.config.id2label[0]\n",
    "# inputs[\"labels\"] = torch.tensor([modeltest.config.label2id[target_label]])\n",
    "# loss = model(**inputs).loss\n",
    "# print(round(loss.item(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASetFineAnyAST(nn.Module):\n",
    "\n",
    "    def __init__(self, inp_t=512, inp_f=128, n_classes=50):\n",
    "        super(ASetFineAnyAST, self).__init__()\n",
    "\n",
    "        self.backbone = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "        # Note that the embedding dim is 768 from the pretrained model\n",
    "\n",
    "\n",
    "        # Replacing the old head for the new classes\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=(768,), eps=1e-12, elementwise_affine=True),\n",
    "            nn.Linear(in_features=768, out_features=n_classes, bias=True)\n",
    "            )\n",
    "        # Modifying the positional embeddings\n",
    "\n",
    "        H_old = (1024 - 16) // 10 + 1\n",
    "        W_old = (128  - 16) // 10 + 1\n",
    "        H_new = (inp_t - 16) // 10 + 1\n",
    "        W_new = (inp_f  - 16) // 10 + 1\n",
    "        \n",
    "        N_old = H_old * W_old\n",
    "        N_new = H_new * W_new\n",
    "\n",
    "        old_pos = self.backbone.audio_spectrogram_transformer.embeddings.position_embeddings.data\n",
    "        \n",
    "        # (CLS-pos, patch-pos grid, DIST-pos):\n",
    "        cls_pe   = old_pos[:, 0, :]              \n",
    "        patch_pe = old_pos[:, 1 : 1 + N_old, :]   \n",
    "        dist_pe  = old_pos[:, 1 +N_old , :]      \n",
    "\n",
    "\n",
    "        # Reshaping patch_pe  [1, D, H_old, W_old] for interpolation to [1, D, H_new, W_new]:\n",
    "        D = patch_pe.size(-1)\n",
    "        patch_pe = patch_pe.view(1, H_old, W_old, D).permute(0, 3, 1, 2)  # [1, D, H_old, W_old]\n",
    "\n",
    "        patch_pe = F.interpolate(\n",
    "            patch_pe,\n",
    "            size=(H_new, W_new),\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "        patch_pe = patch_pe.permute(0, 2, 3, 1).reshape(1, N_new, D)     # [1, N_new, D]\n",
    "\n",
    "        # print(cls_pe.shape, patch_pe.shape, dist_pe.shape)\n",
    "        # Concat back CLS-pos and DIST-pos\n",
    "        new_pos = torch.cat([cls_pe.unsqueeze_(0), patch_pe, dist_pe.unsqueeze_(0)], dim=1)         # [1, 1+N_new+1, D]\n",
    "\n",
    "\n",
    "        # Overwrite the original embeddings\n",
    "        self.backbone.audio_spectrogram_transformer.embeddings.position_embeddings = nn.Parameter(new_pos, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "    \n",
    "        return self.backbone.forward(x).logits\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wav_dir = \"AST/ast-master/egs/esc50/data/ESC-50-master/audio\"\n",
    "# config_path = \"AST/ast-master/egs/esc50/data/datafiles/esc50_config.json\"\n",
    "config_dict = {'num_mel_bins': 128,\n",
    "            'target_length': 512, # {'audioset':1024, 'esc50':512, 'speechcommands':128}\n",
    "            'loss' : 'CE',\n",
    "            'mode':'train', \n",
    "            'mean':-6.6268077, # ESC -6.6268077\n",
    "            'std' : 5.358466, # ESC 5.358466\n",
    "            'fstride' : 10,\n",
    "            'tstride' : 10,\n",
    "            'input_fdim' : 128,\n",
    "            'input_tdim' : 512,\n",
    "            'imagenet_pretrain' : True,\n",
    "            'audioset_pretrain' : True,\n",
    "            'model_size' : 'base384',\n",
    "            'epochs' : 5,\n",
    "            'lr' : 1e-5, # if audioset pretrain is false, then value one order up (1e-4)\n",
    "            'weight_decay' : 5e-7,\n",
    "            'betas' : (0.95, 0.999),\n",
    "            'lrscheduler_start' : 5,\n",
    "            'lrscheduler_step' : 1,\n",
    "            'lrscheduler_decay' : 0.85,\n",
    "            'print_freq' : 100,\n",
    "            'exp_dir' : \"./exp/landing\"\n",
    "}\n",
    "\n",
    "eval_json_path = \"./data/datafiles_fbank/esc_eval_data_1.json\"\n",
    "train_json_path = \"./data/datafiles_fbank/esc_train_data_1.json\"\n",
    "csv_path = \"./data/esc_class_labels_indices.csv\"\n",
    "\n",
    "eval_loader = DataLoader(combined_utils.FbankDataset(eval_json_path, csv_path, config_dict),\n",
    "                          batch_size=4, shuffle=True, num_workers=0, pin_memory=True)\n",
    "train_loader = DataLoader(combined_utils.FbankDataset(train_json_path, csv_path, config_dict),\n",
    "                           batch_size=4, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# temp_train_dataset = combined.FbankDataset(train_json_path, csv_path, config_dict)\n",
    "\n",
    "# os.makedirs(config_dict['exp_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ASetFineAnyAST(inp_t=512, inp_f=128, n_classes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Epoch 1/5\n",
      "Initiating training batches\n",
      "Epoch: [0][0/400]\tTime 0.543 (0.543)\tData 0.009 (0.009)\tLoss 4.2201 (4.2201)\n",
      "Epoch: [0][100/400]\tTime 0.634 (0.625)\tData 0.010 (0.013)\tLoss 2.1011 (3.7279)\n",
      "Epoch: [0][200/400]\tTime 0.629 (0.633)\tData 0.009 (0.013)\tLoss 1.3429 (3.0048)\n",
      "Epoch: [0][300/400]\tTime 0.336 (0.579)\tData 0.016 (0.013)\tLoss 2.1317 (2.4411)\n",
      "Validation: [0/100]\tTime 0.137 (0.137)\tLoss 1.0083 (1.0083)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\Py\\Ai\\IEEE comsoc\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.0757\n",
      "Val Loss: 0.6812\n",
      "Val Accuracy: 0.9100\n",
      "\n",
      "Epoch 2/5\n",
      "Initiating training batches\n",
      "Epoch: [1][0/400]\tTime 0.696 (0.696)\tData 0.000 (0.000)\tLoss 0.2690 (0.2690)\n",
      "Epoch: [1][100/400]\tTime 0.384 (0.400)\tData 0.032 (0.024)\tLoss 0.1021 (0.4951)\n",
      "Epoch: [1][200/400]\tTime 0.408 (0.407)\tData 0.034 (0.028)\tLoss 0.0991 (0.4043)\n",
      "Epoch: [1][300/400]\tTime 0.946 (0.478)\tData 0.013 (0.026)\tLoss 0.3112 (0.3390)\n",
      "Validation: [0/100]\tTime 0.135 (0.135)\tLoss 0.2563 (0.2563)\n",
      "Train Loss: 0.3097\n",
      "Val Loss: 0.3435\n",
      "Val Accuracy: 0.9350\n",
      "\n",
      "Epoch 3/5\n",
      "Initiating training batches\n",
      "Epoch: [2][0/400]\tTime 0.509 (0.509)\tData 0.000 (0.000)\tLoss 0.0640 (0.0640)\n",
      "Epoch: [2][100/400]\tTime 0.441 (0.398)\tData 0.038 (0.023)\tLoss 0.0394 (0.1084)\n",
      "Epoch: [2][200/400]\tTime 0.931 (0.485)\tData 0.012 (0.024)\tLoss 0.0399 (0.0980)\n",
      "Epoch: [2][300/400]\tTime 0.486 (0.626)\tData 0.002 (0.019)\tLoss 0.0532 (0.0814)\n",
      "Validation: [0/100]\tTime 0.161 (0.161)\tLoss 0.4465 (0.4465)\n",
      "Train Loss: 0.0751\n",
      "Val Loss: 0.2575\n",
      "Val Accuracy: 0.9450\n",
      "\n",
      "Epoch 4/5\n",
      "Initiating training batches\n",
      "Epoch: [3][0/400]\tTime 0.675 (0.675)\tData 0.014 (0.014)\tLoss 0.0244 (0.0244)\n",
      "Epoch: [3][100/400]\tTime 0.957 (0.594)\tData 0.000 (0.022)\tLoss 0.0188 (0.0349)\n",
      "Epoch: [3][200/400]\tTime 0.938 (0.770)\tData 0.003 (0.015)\tLoss 0.0220 (0.0371)\n",
      "Epoch: [3][300/400]\tTime 0.410 (0.724)\tData 0.025 (0.014)\tLoss 0.0339 (0.0326)\n",
      "Validation: [0/100]\tTime 0.157 (0.157)\tLoss 0.1799 (0.1799)\n",
      "Train Loss: 0.0304\n",
      "Val Loss: 0.2275\n",
      "Val Accuracy: 0.9550\n",
      "\n",
      "Epoch 5/5\n",
      "Initiating training batches\n",
      "Epoch: [4][0/400]\tTime 0.865 (0.865)\tData 0.000 (0.000)\tLoss 0.0137 (0.0137)\n",
      "Epoch: [4][100/400]\tTime 0.931 (0.947)\tData 0.010 (0.008)\tLoss 0.0121 (0.0180)\n",
      "Epoch: [4][200/400]\tTime 0.943 (0.949)\tData 0.000 (0.008)\tLoss 0.0131 (0.0172)\n",
      "Epoch: [4][300/400]\tTime 0.445 (0.830)\tData 0.026 (0.009)\tLoss 0.0208 (0.0161)\n",
      "Validation: [0/100]\tTime 0.317 (0.317)\tLoss 0.0391 (0.0391)\n",
      "Train Loss: 0.0157\n",
      "Val Loss: 0.2113\n",
      "Val Accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "combined_utils.train_model(model, train_loader, eval_loader, config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = torch.load(\"exp/best_5epoch.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(check['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference: 100%|██████████| 1600/1600 [01:00<00:00, 26.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference complete!\n",
      "Overall accuracy: 1.0000\n",
      "Results saved to: exp/landing/fold1_train_soft_labels.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_utils.run_inference(model, train_json_path, csv_path,'exp/landing/fold1_train_soft_labels.csv', 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
